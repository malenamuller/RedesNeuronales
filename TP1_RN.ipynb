{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Redes Neuronales 2021 \r\n",
    "Integrantes de grupo:\r\n",
    "- Crespo, Pilar\r\n",
    "- Müller, Malena\r\n",
    "- Scala, Tobías\r\n",
    "## TP1: Sesgos en el dataset de SNLI\r\n",
    "\r\n",
    "Uno de los datasets más famosos de Natural Language Inference es SNLI. En esta tarea se debe responder, dadas dos frases A y B, si B es implicación de A (\"entailment\"), B es contradictorio con A (\"contradiction\") o si lo que enuncia B es neutral respecto de A (\"neutral\"). Se dice que A es la premisa y B es la hipótesis.\r\n",
    "\r\n",
    "En Gururangan et al., 2018 mostraron que este dataset tiene algunos sesgos, provocados por ejemplo por las heurísticas que tienen los humanos para generar estos pares de frases (A, B). Para ello, desarrollaron un modelo que aún sin observar la premisa A pudiera clasificar el par (A, B) en alguna de las tres clases del dataset.\r\n",
    "\r\n",
    "En este trabajo práctico intentaremos predecir a qué clase pertenece cada una de las hipótesis sin observar la premisa."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importación de datos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import nltk\r\n",
    "# from nltk import data\r\n",
    "# from nltk.tokenize import word_tokenize\r\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer\r\n",
    "# from nltk.corpus import stopwords\r\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "# import json\r\n",
    "\r\n",
    "# nltk.download('wordnet')\r\n",
    "# nltk.download('punkt')\r\n",
    "# nltk.download('stopwords')\r\n",
    "# lemmatizer = WordNetLemmatizer()\r\n",
    "# stemmer = PorterStemmer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importamos el archivo original del dataset para el procesamiento de datos y nos quedamos con las labels y las segundas oraciones de cada caso, ya que son las hipótesis. La clasificación de a qué clase pertenece cada par de oraciones se hace a partir del análisis de la oración que es la hipótesis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# file = pd.read_json('snli_1.0_train.jsonl', lines=True)\r\n",
    "# fileClass = file['gold_label']\r\n",
    "# fileLines = file['sentence2']\r\n",
    "# #print(fileClass)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filtrado / procesamiento de datos \r\n",
    "\r\n",
    "Tenemos dos tipos de data set. En uno realizamos lo siguiente:\r\n",
    "- word_tokenize: Separamos la oración en strings.\r\n",
    "- isalpha: Se eliminan los números.\r\n",
    "- lemmatize: Se pasa todo a singular y se generaliza el género.\r\n",
    "- stem: Se pasan todos los verbos a infinitivo y se deja todo en minúscula.\r\n",
    "\r\n",
    "Y el otro dataset lo obtenemos de la misma forma que el recién mencionado, pero agregándole también la eliminación de stopwords (palabras comunes).\r\n",
    "\r\n",
    "Si bien inicialmente probamos eliminando las stopwords, determinamos que convenía no hacerlo ya que hay palabras comunes como \"no\", que serían eliminadas, pero en nuestro caso son necesarias estas palabras para determinar que una hipótesis corresponde a una \"contradicción\", por ejemplo. Para esto, nos basamos en el paper \"Annotation Artifacts in Natural Language Inference Data\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# linesFilt = []\r\n",
    "# for i in range(len(fileLines)):\r\n",
    "#     if (i % 1000 == 0):\r\n",
    "#         print(i)\r\n",
    "#     tok=word_tokenize(fileLines[i]) #Separa la oración en strings.\r\n",
    "#     alpha=[x for x in tok if x.isalpha()] #Saca palabras con números.\r\n",
    "#     lem=[lemmatizer.lemmatize(x,pos='v') for x in alpha] #Pasa de plural a singular y generaliza el género.\r\n",
    "#     #stop=[x for x in lem if x not in stopwords.words('english')] #Saca las palabras comunes.\r\n",
    "#     stem=[stemmer.stem(x) for x in lem] #Verbos a infinitivo y pasa todo a minuscula.\r\n",
    "#     linesFilt.append(\" \".join(stem))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Guardamos en un json las hipotesis ya procesadas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# with open('train_processed_.jsonl', 'w') as file:\r\n",
    "#     for i in range(len(fileClass)):\r\n",
    "#         data2jsonl = {'gold_label': fileClass[i], 'sentence2': linesFilt[i]}\r\n",
    "#         json.dump(data2jsonl, file)\r\n",
    "#         file.write('\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repetimos lo anterior para los datasets de validación y de test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# ---- Validation file:\r\n",
    "# file = pd.read_json('snli_1.0_dev.jsonl', lines=True)\r\n",
    "# fileClass = file['gold_label']\r\n",
    "# fileLines = file['sentence2']\r\n",
    "# print(fileClass)\r\n",
    "\r\n",
    "# linesFilt = []\r\n",
    "# for i in range(len(fileLines)):\r\n",
    "#     if (i % 1000 == 0):\r\n",
    "#         print(i)\r\n",
    "#     tok=word_tokenize(fileLines[i]) #Separa la oración en strings.\r\n",
    "#     alpha=[x for x in tok if x.isalpha()] #Saca palabras con números.\r\n",
    "#     lem=[lemmatizer.lemmatize(x,pos='v') for x in alpha] #Pasa de plural a singular y generaliza el género.\r\n",
    "#     #stop=[x for x in lem if x not in stopwords.words('english')] #Saca las palabras comunes.\r\n",
    "#     stem=[stemmer.stem(x) for x in lem] #Verbos a infinitivo y pasa todo a minuscula.\r\n",
    "#     linesFilt.append(\" \".join(stem))\r\n",
    "\r\n",
    "# with open('val_processed_.jsonl', 'w') as file:\r\n",
    "#     for i in range(len(fileClass)):\r\n",
    "#         data2jsonl = {'gold_label': fileClass[i], 'sentence2': linesFilt[i]}\r\n",
    "#         json.dump(data2jsonl, file)\r\n",
    "#         file.write('\\n')\r\n",
    "\r\n",
    "# ---- Test file:\r\n",
    "# file = pd.read_json('snli_1.0_test.jsonl', lines=True)\r\n",
    "# fileClass = file['gold_label']\r\n",
    "# fileLines = file['sentence2']\r\n",
    "# print(fileClass)\r\n",
    "\r\n",
    "# linesFilt = []\r\n",
    "# for i in range(len(fileLines)):\r\n",
    "#     if (i % 1000 == 0):\r\n",
    "#         print(i)\r\n",
    "#     tok=word_tokenize(fileLines[i]) #Separa la oración en strings.\r\n",
    "#     alpha=[x for x in tok if x.isalpha()] #Saca palabras con números.\r\n",
    "#     lem=[lemmatizer.lemmatize(x,pos='v') for x in alpha] #Pasa de plural a singular y generaliza el género.\r\n",
    "#     #stop=[x for x in lem if x not in stopwords.words('english')] #Saca las palabras comunes.\r\n",
    "#     stem=[stemmer.stem(x) for x in lem] #Verbos a infinitivo y pasa todo a minuscula.\r\n",
    "#     linesFilt.append(\" \".join(stem))\r\n",
    "\r\n",
    "# with open('test_processed_.jsonl', 'w') as file:\r\n",
    "#     for i in range(len(fileClass)):\r\n",
    "#         data2jsonl = {'gold_label': fileClass[i], 'sentence2': linesFilt[i]}\r\n",
    "#         json.dump(data2jsonl, file)\r\n",
    "#         file.write('\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrenamiento de la red con Naive Bayes\r\n",
    "\r\n",
    "Levantamos los datos ya procesados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd #Implementación de clasificador bayesiano.\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\r\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\r\n",
    "\r\n",
    "trainFile = pd.read_json('train_processed_.jsonl', lines=True)\r\n",
    "trainClass = trainFile['gold_label'].tolist()\r\n",
    "trainLines = trainFile['sentence2'].tolist()\r\n",
    "\r\n",
    "valFile = pd.read_json('val_processed_.jsonl', lines=True)\r\n",
    "valClass = valFile['gold_label'].tolist()\r\n",
    "valLines = valFile['sentence2'].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Obtenemos las matrices dispersas que contienen la frecuencia (cantidad de ocurrencia) de cada palabra del vocabulario."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# countVect = CountVectorizer(max_df=0.8,min_df=10, ngram_range=(1,2)) #Recibe todos los artículos y arma los vectores de cuenta. Check \"ngram_range\"\r\n",
    "tfidfVect = TfidfVectorizer(max_df=0.8,min_df=5, ngram_range=(1,2))#, max_features=1000) #As tf–idf is very often used for text features\r\n",
    "\r\n",
    "# trainData = countVect.fit_transform(trainLines) #Learn a vocabulary dictionary of all tokens in the raw documents and return document-term matrix.\r\n",
    "# print(trainData.shape) #En este sparse matrix se muestran las ocurrencias de cada palabra en cada linea.\r\n",
    "trainData = tfidfVect.fit_transform(trainLines) #Learn vocabulary and idf from training set, return document-term matrix.\r\n",
    "print(trainData.shape) #En este sparse matrix se muestran las ocurrencias de cada palabra en cada linea. Con tfidf las ocurrencias de las palabras no se representan con números enteros.\r\n",
    "\r\n",
    "# valData = countVect.transform(valLines) #Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit.\r\n",
    "# print(valData.shape)\r\n",
    "valData = tfidfVect.transform(valLines) #Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit.\r\n",
    "print(valData.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(550152, 56323)\n",
      "(10000, 56323)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos el modelo utilizando Naive Bayes y obtenemos el score con los datos de train y el score con los nuevos datos (validation)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "multiNB = MultinomialNB(alpha=0.8)\r\n",
    "multiNB.fit(trainData, trainClass)\r\n",
    "\r\n",
    "print(multiNB.score(trainData, trainClass))\r\n",
    "print(multiNB.score(valData, valClass))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6645381639983132\n",
      "0.6414\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comprobamos la efectividad del modelo prediciendo los nuevos datos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from sklearn.metrics import precision_score\r\n",
    "from keras.utils import np_utils\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "\r\n",
    "valPred = multiNB.predict(valData)\r\n",
    "\r\n",
    "enc = LabelEncoder()\r\n",
    "enc.fit(valClass)\r\n",
    "valClass = enc.transform(valClass)\r\n",
    "valClass = np_utils.to_categorical(valClass-1) #Convert integers to dummy variables (i.e. one hot encoded).\r\n",
    "enc.fit(valPred)\r\n",
    "valPred = enc.transform(valPred)\r\n",
    "valPred = np_utils.to_categorical(valPred) #Convert integers to dummy variables (i.e. one hot encoded).\r\n",
    "\r\n",
    "print(precision_score(valClass, valPred, average=None))\r\n",
    "# print(precision_score(valClass, valPred, average='micro'))\r\n",
    "# print(precision_score(valClass, valPred, average='macro'))\r\n",
    "# print(precision_score(valClass, valPred, average='weighted'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.63833581 0.63371105 0.67246377]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusiones de Multinomial Naive Bayes\r\n",
    "\r\n",
    "..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrenamiento de la red con MLP\r\n",
    "\r\n",
    "Truncamos el vocabulario para no tener error por falta de memoria."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.optimizers import Adam, SGD\r\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "\r\n",
    "trunSVD = TruncatedSVD(n_components=1000)\r\n",
    "trainData = trunSVD.fit_transform(trainData)\r\n",
    "valData = trunSVD.transform(valData)\r\n",
    "print(trainData.shape)\r\n",
    "\r\n",
    "# enc = LabelEncoder()\r\n",
    "enc.fit(trainClass)\r\n",
    "trainClass = enc.transform(trainClass)\r\n",
    "trainClass = np_utils.to_categorical(trainClass-1) #Convert integers to dummy variables (i.e. one hot encoded).\r\n",
    "#print(trainClass_)\r\n",
    "# valClass = enc.transform(valClass)\r\n",
    "# valClass = np_utils.to_categorical(valClass-1) #Convert integers to dummy variables (i.e. one hot encoded)."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(550152, 1000)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generamos la estructura de la red neuronal con la siguiente función."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def neuralNetwork():\r\n",
    "    Nwords = trainData.shape[1] #cantidad de datos (palabras del vocabulario). ###\r\n",
    "    model = Sequential()\r\n",
    "    model.add(Dense(200, input_shape=(Nwords,), activation='relu'))\r\n",
    "    model.add(Dense(trainClass.shape[1], activation='softmax')) #La salida de la función softmax puede ser utilizada para representar una distribución categórica. \r\n",
    "                                                #Es empleada en varios métodos de clasificación multiclase tales como Regresión Logística Multinomial.\r\n",
    "                                                #La función softmax es utilizada como capa final de los clasificadores basados en redes neuronales.\r\n",
    "    model.summary()\r\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos la red neuronal y obtenemos el accuracyy del mismo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "#Cross validation.\r\n",
    "# estClass = KerasClassifier(build_fn=neuralNetwork, epochs=50, batch_size=256, verbose=0) #Estimator.\r\n",
    "# Kfold = KFold(n_splits=10, shuffle=True)\r\n",
    "# scores = cross_val_score(estClass, trainData_.todense(), encClass_, cv=Kfold)\r\n",
    "# print(\"Score: %.2f%% (%.2f%%)\" % (scores.mean()*100, scores.std()*100))\r\n",
    "#Hold out\r\n",
    "model = neuralNetwork()\r\n",
    "model.fit(trainData, trainClass, epochs=10, batch_size=256, verbose=1)\r\n",
    "loss, acc = model.evaluate(valData, valClass)\r\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 200)               200200    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 200,803\n",
      "Trainable params: 200,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2150/2150 [==============================] - 9s 4ms/step - loss: 0.8663 - accuracy: 0.6032\n",
      "Epoch 2/10\n",
      "2150/2150 [==============================] - 10s 5ms/step - loss: 0.8338 - accuracy: 0.6216\n",
      "Epoch 3/10\n",
      "2150/2150 [==============================] - 9s 4ms/step - loss: 0.8131 - accuracy: 0.6354\n",
      "Epoch 4/10\n",
      "2150/2150 [==============================] - 8s 4ms/step - loss: 0.7946 - accuracy: 0.6466\n",
      "Epoch 5/10\n",
      "2150/2150 [==============================] - 7s 3ms/step - loss: 0.7790 - accuracy: 0.6555\n",
      "Epoch 6/10\n",
      "2150/2150 [==============================] - 9s 4ms/step - loss: 0.7652 - accuracy: 0.6628\n",
      "Epoch 7/10\n",
      "2150/2150 [==============================] - 7s 3ms/step - loss: 0.7527 - accuracy: 0.6699\n",
      "Epoch 8/10\n",
      "2150/2150 [==============================] - 8s 4ms/step - loss: 0.7415 - accuracy: 0.6759\n",
      "Epoch 9/10\n",
      "2150/2150 [==============================] - 8s 4ms/step - loss: 0.7307 - accuracy: 0.6817\n",
      "Epoch 10/10\n",
      "2150/2150 [==============================] - 8s 4ms/step - loss: 0.7209 - accuracy: 0.6868\n",
      "313/313 [==============================] - 0s 748us/step - loss: 0.8188 - accuracy: 0.6392\n",
      "Test Accuracy: 63.919997\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comprobamos la eficiencia del modelo con los nuevos datos (validation)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "valPred = model.predict(valData, verbose = 1).round()\r\n",
    "\r\n",
    "print(precision_score(valClass, valPred, average=None))\r\n",
    "# print(precision_score(valClass_, valPred, average='micro'))\r\n",
    "# print(precision_score(valClass_, valPred, average='macro'))\r\n",
    "# print(precision_score(valClass_, valPred, average='weighted'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 0s 528us/step\n",
      "[0.71124744 0.67965217 0.68814815]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusiones de MLP\r\n",
    "\r\n",
    "..."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('rn': conda)"
  },
  "interpreter": {
   "hash": "352c8700b9e48b360dd8f8c948f408188979ddb3d67f3cf144936d3d11d35124"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}